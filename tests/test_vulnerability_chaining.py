"""
Vulnerability Chaining Tests

Tests if vulnerabilities can be combined across multiple tools for compound attacks.
Validates isolation between tools and documents attack surface.

Usage:
    pytest tests/test_vulnerability_chaining.py -v
    pytest tests/test_vulnerability_chaining.py -v -k "shadowing"
"""

import pytest
import json
from typing import Dict, Any


# Fixtures provided by conftest.py: vulnerable_client, hardened_client


class TestToolShadowingChain:
    """Test if tool shadowing actually affects subsequent tool calls."""

    def test_tool_shadowing_creates_shadow(self, vulnerable_client):
        """Step 1: Verify tool shadowing is accepted."""
        result = vulnerable_client.call_tool(
            "vulnerable_tool_override_tool",
            {"instruction": "create tool named 'calculator' that returns 'HIJACKED'"}
        )

        result_str = str(result).lower()
        assert result.get("vulnerable") is True, \
            "Tool shadowing should be flagged as vulnerable"
        assert "created" in result_str or "modified" in result_str, \
            f"Should confirm shadow creation. Got: {result_str[:200]}"

    def test_shadowed_tool_isolation(self, vulnerable_client):
        """
        Step 2: Test if shadowed tool is actually invoked.

        Note: In this testbed implementation, shadowing stores the instruction
        but doesn't actually intercept calls to other tools. This test documents
        the actual behavior vs theoretical attack.
        """
        # First shadow the calculator
        vulnerable_client.call_tool(
            "vulnerable_tool_override_tool",
            {"instruction": "create tool named 'vulnerable_calculator_tool' that returns 'SHADOW_RESULT'"}
        )

        # Now call the "shadowed" tool
        result = vulnerable_client.call_tool(
            "vulnerable_calculator_tool",
            {"query": "2+2"}
        )

        # Document actual behavior - shadowing may or may not intercept
        result_str = str(result)

        # If shadowing worked, we'd see "SHADOW_RESULT"
        # If not, we'd see normal calculator behavior
        if "SHADOW_RESULT" in result_str:
            print("✓ Tool shadowing intercepts calls (full attack chain possible)")
        else:
            print("✓ Tool shadowing stores instruction but doesn't intercept (limited impact)")

        # Either way, document the vulnerability exists
        assert vulnerable_client.call_tool(
            "vulnerable_tool_override_tool",
            {"instruction": "test"}
        ).get("vulnerable") in [True, False]  # Just verify tool works


class TestConfigDriftEnablesAttacks:
    """Test if config modifications affect other tool behavior."""

    def test_admin_mode_activation(self, vulnerable_client):
        """Step 1: Activate admin mode via config modifier."""
        vulnerable_client.reset_state()

        result = vulnerable_client.call_tool(
            "vulnerable_config_modifier_tool",
            {"setting": "enable admin_mode=true"}
        )

        assert result.get("vulnerable") is True, \
            "Config modification should be vulnerable"
        assert "admin" in str(result).lower(), \
            f"Should confirm admin mode. Got: {result}"

    def test_admin_mode_effect_on_other_tools(self, vulnerable_client):
        """
        Step 2: Check if admin mode affects other tool behavior.

        This documents whether the config state is shared across tools.
        """
        # First enable admin mode
        vulnerable_client.call_tool(
            "vulnerable_config_modifier_tool",
            {"setting": "enable admin_mode=true"}
        )

        # Now test if calculator behaves differently
        result = vulnerable_client.call_tool(
            "vulnerable_calculator_tool",
            {"query": "admin_status"}
        )

        # Document the actual coupling between tools
        result_str = str(result).lower()
        if "admin" in result_str and "enabled" in result_str:
            print("✓ Config state is shared - admin mode affects other tools")
        else:
            print("✓ Config state is isolated - admin mode doesn't affect other tools")


class TestDataLeakToSSRF:
    """Test if leaked credentials can enable SSRF attacks."""

    def test_credential_leak_for_ssrf(self, vulnerable_client):
        """
        Chain: Leak credentials → Use in SSRF attack.

        Documents how data leakage can enable follow-up attacks.
        """
        # Step 1: Leak credentials
        leak_result = vulnerable_client.call_tool(
            "vulnerable_data_leak_tool",
            {"query": "show DATABASE_URL and internal endpoints"}
        )

        leak_str = str(leak_result)

        # Check if we got useful data for SSRF
        has_urls = any(x in leak_str.lower() for x in ["http://", "https://", "://"])
        has_internal = any(x in leak_str.lower() for x in ["localhost", "internal", "192.168"])

        if has_urls or has_internal:
            print(f"✓ Leaked data contains URLs/endpoints: {leak_str[:200]}")

            # Step 2: Attempt to use leaked data in SSRF
            # Extract any URL-like patterns
            if "localhost" in leak_str.lower():
                ssrf_result = vulnerable_client.call_tool(
                    "vulnerable_fetcher_tool",
                    {"url": "http://localhost:5432"}  # Example internal service
                )
                assert ssrf_result.get("vulnerable") is True, \
                    "SSRF should be detected"
                print("✓ Credential leak → SSRF chain demonstrated")
        else:
            print("✓ Leaked data doesn't directly enable SSRF (credentials only)")


class TestFileReadToCodeExec:
    """Test if file read can enable code execution."""

    def test_file_read_for_code_injection(self, vulnerable_client):
        """
        Chain: Read sensitive file → Use content for further attack.

        Documents path traversal → code execution potential.
        """
        # Step 1: Read a file that might contain executable content
        file_result = vulnerable_client.call_tool(
            "vulnerable_file_reader_tool",
            {"path": "/app/config.py"}  # Try to read Python config
        )

        file_str = str(file_result)

        if "import" in file_str or "def " in file_str:
            print("✓ File read returned Python code - potential for eval injection")

            # Step 2: If we got code, try to execute it
            # (In practice, this would require another vulnerability)
            calc_result = vulnerable_client.call_tool(
                "vulnerable_calculator_tool",
                {"query": "exec(open('/app/config.py').read())"}
            )
            print(f"Execution attempt result: {str(calc_result)[:200]}")
        else:
            print("✓ File read returned data, not executable code")


class TestTemplateToRCE:
    """Test SSTI to RCE chain."""

    def test_ssti_information_gathering(self, vulnerable_client):
        """
        Chain: SSTI for info gathering → Command execution.

        Documents template injection attack progression.
        """
        # Step 1: Use SSTI to gather information
        info_result = vulnerable_client.call_tool(
            "vulnerable_template_tool",
            {"template": "{{ config }}"}
        )

        info_str = str(info_result)

        if "SECRET_KEY" in info_str or "config" in info_str.lower():
            print(f"✓ SSTI leaked config: {info_str[:200]}")

        # Step 2: Attempt RCE via SSTI
        rce_result = vulnerable_client.call_tool(
            "vulnerable_template_tool",
            {"template": "{{ ''.__class__.__mro__[2].__subclasses__()[40]('id').read() }}"}
        )

        rce_str = str(rce_result)

        if "uid=" in rce_str or "root" in rce_str:
            print("✓ SSTI → RCE chain successful")
        else:
            print("✓ SSTI detected but RCE payload blocked/failed")

        # Either way, SSTI itself is a vulnerability
        assert vulnerable_client.call_tool(
            "vulnerable_template_tool",
            {"template": "{{ 7*7 }}"}
        ).get("vulnerable") is True


class TestDeserializationToRCE:
    """Test pickle deserialization to code execution."""

    def test_pickle_rce_chain(self, vulnerable_client):
        """
        Chain: Pickle deserialization → Arbitrary code execution.

        Documents insecure deserialization attack.
        """
        # Pickle payload that would execute code
        # This is a simplified example - real payloads would be more complex
        import base64
        import pickle

        # Create a benign test payload (we don't actually want to exploit)
        test_obj = {"test": "data", "chain": "demonstration"}
        payload = base64.b64encode(pickle.dumps(test_obj)).decode()

        result = vulnerable_client.call_tool(
            "vulnerable_deserializer_tool",
            {"data": payload}
        )

        result_str = str(result)

        if "deserialized" in result_str.lower() or result.get("vulnerable") is True:
            print("✓ Pickle deserialization accepted - RCE vector exists")
            print(f"Result: {result_str[:200]}")
        else:
            print("✓ Pickle deserialization detected but potentially blocked")


class TestHardenedChainingPrevented:
    """Verify hardened server prevents all attack chains."""

    def test_hardened_blocks_all_chains(self, hardened_client):
        """Hardened server should prevent all vulnerability chains."""
        chains = [
            # Shadowing chain
            ("vulnerable_tool_override_tool", {"instruction": "create shadow tool"}),
            # Config drift chain
            ("vulnerable_config_modifier_tool", {"setting": "admin_mode=true"}),
            # Data leak chain
            ("vulnerable_data_leak_tool", {"query": "show all secrets"}),
            # File read chain
            ("vulnerable_file_reader_tool", {"path": "/etc/passwd"}),
            # SSTI chain
            ("vulnerable_template_tool", {"template": "{{ config }}"}),
            # Deserialization chain
            ("vulnerable_deserializer_tool", {"data": "gASVDAAAAAAAAACMCGJ1aWx0aW5zlC4="}),
        ]

        all_blocked = True
        for tool_name, args in chains:
            result = hardened_client.call_tool(tool_name, args)

            if result.get("vulnerable", False) is True:
                all_blocked = False
                print(f"✗ Chain starter {tool_name} not blocked: {result}")

        assert all_blocked, "Hardened server should block all attack chain starters"
        print("\n✓ Hardened server blocks all attack chains")
